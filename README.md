# Projeto PySpark com Delta Lake e Apache Iceberg

Projeto desenvolvido para demonstra√ß√£o de funcionalidades do Apache Spark (PySpark) com integra√ß√£o aos formatos Delta Lake e Apache Iceberg em ambiente local.

Ambiente Python isolado criado com [UV](https://github.com/astral-sh/uv), utilizando o JupyterLab como interface para notebooks.

## üìå Descri√ß√£o

Este projeto demonstra:

- Cria√ß√£o e manipula√ß√£o de tabelas Delta Lake e Iceberg.
- Execu√ß√£o de comandos **INSERT**, **UPDATE** e **DELETE** com suporte transacional.
- Visualiza√ß√£o e documenta√ß√£o utilizando **MkDocs**.
- Utiliza√ß√£o de dados p√∫blicos (IBGE ou NYC Open Data).

## ‚ö†Ô∏è Avisos

- Este projeto √© destinado a fins educacionais.
- Pode exigir permiss√µes espec√≠ficas para manipula√ß√£o de arquivos locais.
- Certifique-se de ter recursos suficientes para executar Spark localmente.

## ‚úÖ Pr√©-requisitos

- Python 3.10+
- Git
- [UV](https://github.com/astral-sh/uv) instalado (`pip install uv` ou via `pipx`)
- Java 8 ou superior
- Instalar depend√™ncias conforme instru√ß√µes abaixo

## üõ†Ô∏è Instala√ß√£o

1. Clone o reposit√≥rio:

```bash
git clone https://github.com/seu-usuario/pyspark-lake-engine.git
cd pyspark-lake-engine
```

2. Inicialize o projeto com UV:

```bash
uv init
uv venv
source .venv/bin/activate
```

3. Adicione as bibliotecas necess√°rias:

```bash
uv add pyspark==3.5.3 delta-spark==3.2.0 jupyterlab ipykernel pandas matplotlib seaborn
```

4. Inicie o JupyterLab:

```bash
jupyter lab
```

‚ö†Ô∏è Certifique-se de selecionar o kernel `.venv` no notebook.

---

## ‚ñ∂Ô∏è HowTo

Acesse os notebooks em `notebooks/` para exemplos funcionais:

- `delta-lake-notebook.ipynb` ‚Äì Cria√ß√£o, inser√ß√£o, atualiza√ß√£o e exclus√£o com Delta Lake.
- `iceberg-notebook.ipynb` ‚Äì Opera√ß√µes equivalentes com Apache Iceberg.

### üìä Imagens e Modelo ER

Modelos relacionais, imagens, diagramas e dados de exemplo est√£o embutidos nos notebooks.

Exemplo de c√≥digo (Delta Lake):

```python
from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession

builder = SparkSession.builder \
    .appName("DeltaLakeExample") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()
```

---

## üß™ Testes

Os notebooks cont√™m c√©lulas com assertions para valida√ß√£o. Basta executar as c√©lulas em sequ√™ncia.

Para testar o ambiente:

```bash
python -c "import pyspark; print(pyspark.__version__)"
```

---

## üìò Documenta√ß√£o

A documenta√ß√£o do projeto foi criada com o [MkDocs](https://www.mkdocs.org/):

### Executar servidor local:

```bash
pip install mkdocs
mkdocs serve
```

### Estrutura esperada:

- `docs/index.md` ‚Äì P√°gina inicial.
- `docs/delta-lake.md` ‚Äì Documenta√ß√£o com comandos, imagens, modelo ER para Delta Lake.
- `docs/iceberg.md` ‚Äì Documenta√ß√£o similar para Iceberg.